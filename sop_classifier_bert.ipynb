{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bert_pytorch.model import ALBERT, ALBERTLM\n",
    "from bert_pytorch.trainer import BERTTrainer\n",
    "from bert_pytorch.dataset import ALBERTDataset, WordVocab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "from torch.optim import Adam\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# numpy & pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# scikit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import textlib as tl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = '../TCL2021_Telco_Embedding_Dataset/corpora/telco_vocab.dat'\n",
    "vocab = WordVocab.load_vocab(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = ALBERT(vocab_size=len(vocab), embed_size=128, hidden=256, n_layers=8, attn_heads=8, seq_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bert = torch.load('../TCL2021_Telco_Embedding_Dataset/albert_model_good/albert_model_weights_only_finetuning/albert.model_weightsonly.ep3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2393,  0.0931,  0.2913,  ..., -0.3466, -0.0221,  0.2755],\n",
       "        [-0.6690, -0.1193, -1.0739,  ...,  0.2408,  0.1327,  0.4912],\n",
       "        [-0.4424,  0.0456, -0.6106,  ...,  1.4350,  0.1514, -0.5756],\n",
       "        ...,\n",
       "        [-0.1260, -0.2287,  0.2891,  ...,  0.2943, -0.4238, -0.8864],\n",
       "        [-0.4999,  0.0103,  0.0177,  ...,  0.0442, -0.4262,  0.1222],\n",
       "        [ 0.6505,  0.5411, -0.3057,  ...,  0.3251,  0.2935, -0.2772]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.embedding.token.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOPClassifier(nn.Module):\n",
    "    def __init__(self, bert: ALBERT, num_class):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.decode = nn.Linear(self.bert.hidden, num_class)\n",
    "        \n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        out = self.decode(x[:, 0])\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 37\n",
    "batch_size = 64\n",
    "seq_len = 64\n",
    "\n",
    "clf = SOPClassifier(bert, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 37])\n",
      "tensor([[ 0.1755,  0.1799,  0.0135,  ..., -0.1694, -0.0202,  0.3690],\n",
      "        [ 0.1956,  0.1741, -0.0095,  ..., -0.1570, -0.0545,  0.3896],\n",
      "        [ 0.1933,  0.1771,  0.0021,  ..., -0.1937, -0.0451,  0.3882],\n",
      "        ...,\n",
      "        [ 0.1735,  0.1760, -0.0054,  ..., -0.1792, -0.0362,  0.3709],\n",
      "        [ 0.1888,  0.1777,  0.0110,  ..., -0.1755, -0.0439,  0.3807],\n",
      "        [ 0.1847,  0.1820, -0.0056,  ..., -0.1805, -0.0430,  0.3927]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "\n",
    "x = torch.zeros([batch_size,seq_len], dtype=torch.int32)\n",
    "s = torch.ones([batch_size,seq_len], dtype=torch.int32)\n",
    "\n",
    "\n",
    "\n",
    "y = clf.forward(x, s)\n",
    "print(y.shape)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65635, 6)\n"
     ]
    }
   ],
   "source": [
    "# sop dataset 읽어옴\n",
    "input_file_name = '../TCL2021_Telco_Embedding_Dataset/dataset/sop_dataset.xlsx'\n",
    "try:\n",
    "    df = pd.read_excel(input_file_name, sheet_name=0, engine='openpyxl')\n",
    "except FileNotFoundError:\n",
    "    print(f'{input_file_name}이 없습니다! skip!')\n",
    "\n",
    "print( df.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 모델은 sentence와 label만 써보자\n",
    "# df_zip = df[ ['sentence', 'label'] ]\n",
    "\n",
    "y = df.pop('label_clean')\n",
    "X = df.pop('sentence')\n",
    "\n",
    "# 문자열로 되어 있는 label을 categorical value로 변환\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test 분리\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    #train_test_split(X[:1000], y[:1000], test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOPDataset(Dataset):\n",
    "    def __init__(self, X, y, vocab, seq_len):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.X = []\n",
    "        self.y = torch.LongTensor(y)\n",
    "        \n",
    "        print(f'data loading started! size = {len(X)}')\n",
    "        for i, text in enumerate(X):\n",
    "            try:\n",
    "                # 클렌징\n",
    "                cleansed_text = tl.clean_text(text)\n",
    "            except TypeError:\n",
    "                print(f'      {i+1} 번째 데이터에 문제가 있어 skip!')\n",
    "                continue\n",
    "\n",
    "            # 문장으로 분리하여 배열로 리턴\n",
    "            sentences = tl.segment_sentences(cleansed_text)\n",
    "            # 문장 배열을 입릭으로 받아 형태소로 쪼갠 뒤, 다시 하나의 문자열로 변환하여 저장\n",
    "            corpora = ' '.join(tl.get_corpora(sentences)).split(' ')\n",
    "            self.X.append(corpora)   \n",
    "            \n",
    "            if i%1000 == 0:\n",
    "                print(f'{i}th data loading completed!')\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        x = self.X[item]        \n",
    "        x = x[:self.seq_len-2]\n",
    "        # token to id\n",
    "        tokens = ['<sos>'] + x + ['<eos>']\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        \n",
    "        input_ids = self.vocab.to_seq(tokens)\n",
    "        \n",
    "        n_pad = self.seq_len - len(input_ids)\n",
    "        input_ids.extend([0]*n_pad)\n",
    "        segment_ids.extend([0]*n_pad)\n",
    "        \n",
    "        output = {'input_ids': input_ids,\n",
    "                  'segment_ids': segment_ids,\n",
    "                  'label': self.y[item]}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading started! size = 52508\n",
      "0th data loading completed!\n",
      "1000th data loading completed!\n",
      "2000th data loading completed!\n",
      "3000th data loading completed!\n",
      "4000th data loading completed!\n",
      "5000th data loading completed!\n",
      "6000th data loading completed!\n",
      "7000th data loading completed!\n",
      "8000th data loading completed!\n",
      "9000th data loading completed!\n",
      "10000th data loading completed!\n",
      "11000th data loading completed!\n",
      "12000th data loading completed!\n",
      "13000th data loading completed!\n",
      "14000th data loading completed!\n",
      "15000th data loading completed!\n",
      "16000th data loading completed!\n",
      "17000th data loading completed!\n",
      "18000th data loading completed!\n",
      "19000th data loading completed!\n",
      "20000th data loading completed!\n",
      "21000th data loading completed!\n",
      "22000th data loading completed!\n",
      "23000th data loading completed!\n",
      "24000th data loading completed!\n",
      "25000th data loading completed!\n",
      "26000th data loading completed!\n",
      "27000th data loading completed!\n",
      "28000th data loading completed!\n",
      "29000th data loading completed!\n",
      "30000th data loading completed!\n",
      "31000th data loading completed!\n",
      "32000th data loading completed!\n",
      "33000th data loading completed!\n",
      "34000th data loading completed!\n",
      "35000th data loading completed!\n",
      "36000th data loading completed!\n",
      "37000th data loading completed!\n",
      "38000th data loading completed!\n",
      "39000th data loading completed!\n",
      "40000th data loading completed!\n",
      "41000th data loading completed!\n",
      "42000th data loading completed!\n",
      "43000th data loading completed!\n",
      "44000th data loading completed!\n",
      "45000th data loading completed!\n",
      "46000th data loading completed!\n",
      "47000th data loading completed!\n",
      "48000th data loading completed!\n",
      "49000th data loading completed!\n",
      "50000th data loading completed!\n",
      "51000th data loading completed!\n",
      "52000th data loading completed!\n",
      "data loading started! size = 13127\n",
      "0th data loading completed!\n",
      "1000th data loading completed!\n",
      "2000th data loading completed!\n",
      "3000th data loading completed!\n",
      "4000th data loading completed!\n",
      "5000th data loading completed!\n",
      "6000th data loading completed!\n",
      "7000th data loading completed!\n",
      "8000th data loading completed!\n",
      "9000th data loading completed!\n",
      "10000th data loading completed!\n",
      "11000th data loading completed!\n",
      "12000th data loading completed!\n",
      "13000th data loading completed!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SOPDataset(X_train, y_train, vocab, seq_len)\n",
    "test_dataset  = SOPDataset(X_test, y_test, vocab, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True,\n",
    "                            num_workers=1)\n",
    "test_data_loader  = DataLoader(test_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = Adam(clf.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SOPClassifier(\n",
       "  (bert): ALBERT(\n",
       "    (embedding): BERTEmbedding(\n",
       "      (token): Embedding(9544, 128)\n",
       "      (position): Embedding(64, 128)\n",
       "      (segment): Embedding(2, 128)\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (factorized): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (transformer_block): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decode): Linear(in_features=256, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_condition = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(clf, optim, epoch, data_loader, criterion, train=True):\n",
    "    sum_loss = 0.0\n",
    "    total_correct_sop = 0\n",
    "    total_element_sop = 0\n",
    "\n",
    "    str_code = \"train\" if train else \"test\"\n",
    "    \n",
    "    if train:\n",
    "        clf.train()\n",
    "    else:\n",
    "        clf.eval()\n",
    "\n",
    "    data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                            desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                            total=len(data_loader),\n",
    "                            bar_format=\"{l_bar}{r_bar}\")        \n",
    "    \n",
    "    for i, data in data_iter:\n",
    "        # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "        data = {key: value.to(device) for key, value in data.items()}\n",
    "\n",
    "        pred_y = clf.forward(data[\"input_ids\"], data[\"segment_ids\"])\n",
    "\n",
    "        loss = criterion(pred_y, data[\"label\"])\n",
    "\n",
    "        # 3. backward and optimization only in train\n",
    "        if train: \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        # SOP accuracy\n",
    "        correct_sop = pred_y.argmax(dim=-1).eq(data[\"label\"]).sum().item()\n",
    "        total_correct_sop += correct_sop\n",
    "        total_element_sop += data[\"label\"].nelement()\n",
    "\n",
    "        sop_acc = total_correct_sop / total_element_sop * 100\n",
    "\n",
    "        post_fix = {\n",
    "            \"epoch\": \"[%d/%s]\" % (epoch, str_code),\n",
    "            \"iter\": \"[%d/%d]\" % (i, len(data_loader)),\n",
    "            \"avg_loss\": sum_loss / (i + 1),\n",
    "            \"sop_acc\": sop_acc,\n",
    "            \"total_loss\": loss.item()\n",
    "        }\n",
    "\n",
    "        global_step = epoch * len(data_loader) + i\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            data_iter.write(str(post_fix))\n",
    "\n",
    "    print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), sum_loss / len(data_loader), \\\n",
    "        \"total_sop_acc=\", total_correct_sop * 100.0 / total_element_sop)\n",
    "\n",
    "def train(clf, optim, epoch, data_loader, criterion):\n",
    "    iterate(clf, optim, epoch, data_loader, criterion, True)\n",
    "\n",
    "def test(clf, optim, epoch, data_loader, criterion):\n",
    "    iterate(clf, optim, epoch, data_loader, criterion, False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    train(clf, optim, epoch, train_data_loader, criterion)\n",
    "    test (clf, optim, epoch, test_data_loader, criterion)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
