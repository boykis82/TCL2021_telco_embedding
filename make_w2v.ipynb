{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boyki\\anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# my lib\n",
    "import textlib as tl\n",
    "import Word2VecModel as wv\n",
    "import FastTextModel as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 병렬 수행을 위한 core 개수 가져오기 (모든 코어를 사용하면 다른 작업이 불가하여 -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKERS = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 사용 가능 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 말뭉치 경로\n",
    "corpora_prttag_file_name = '../TCL2021_Telco_Embedding_Dataset/corpora/telco_corpora.dat'\n",
    "corpora_alltag_file_name = '../TCL2021_Telco_Embedding_Dataset/corpora/telco_corpora_all_tag.dat'\n",
    "\n",
    "# 일부 tag(명사, 형용사 계열)로만 만든 embedding vector를 저장할 경로\n",
    "w2v_model_prttag_file_name_prefix = '../TCL2021_Telco_Embedding_Dataset/embedding_w2v/telco_w2v_'\n",
    "\n",
    "# 모든 tag로 만든 embedding vector를 저장할 경로\n",
    "w2v_model_alltag_file_name_prefix = '../TCL2021_Telco_Embedding_Dataset/embedding_w2v_alltag/telco_w2v_'\n",
    "\n",
    "# 모든 tag로 만든 fasttext embedding vector를 저장할 경로\n",
    "ft_model_file_name_prefix = '../TCL2021_Telco_Embedding_Dataset/embedding_fasttext/telco_ft_'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일부 형태소 사용할지, 전체 형태소 사용할지 여부에 따라 아래 코드 변경하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v 모든 형태소 사용\n",
    "w2v_model_file_name_prefix = w2v_model_alltag_file_name_prefix\n",
    "corpora_file_name = corpora_alltag_file_name\n",
    "\n",
    "# w2v 일부 형태소 사용\n",
    "#w2v_model_file_name_prefix = w2v_model_prttag_file_name_prefix\n",
    "#corpora_file_name = corpora_prttag_file_name\n",
    "\n",
    "# fasttext\n",
    "#w2v_model_file_name_prefix = ft_model_file_name_prefix\n",
    "#corpora_file_name = corpora_alltag_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 말뭉치 통계 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence.strip().split(' ') \n",
    "                  for sentence in open(corpora_file_name, 'r', encoding='utf-8').readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from itertools import chain\n",
    "fdist = nltk.FreqDist(chain.from_iterable(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000, 40000, 1000):\n",
    "    print( str(i) + '------>' + str(fdist.most_common(i)[-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어수는 10,000개로 고정. embedding 차원수 [128,256,384], window size [3,4,5]의 조합으로 총 9개 embedding 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 여러개의 w2v 모델을 만들기 위한 table\n",
    "MODEL_COUNT = 9\n",
    "\n",
    "W2V_TRAIN_PARAMS = {\n",
    "    'MODEL_NAME': ['V10000_E128_W3','V10000_E128_W4','V10000_E128_W5',\n",
    "                   'V10000_E256_W3','V10000_E256_W4','V10000_E256_W5',\n",
    "                   'V10000_E384_W3','V10000_E384_W4','V10000_E384_W5'],\n",
    "    'MAX_VOCAB_SIZE': [10000] * MODEL_COUNT,\n",
    "    'EMBEDDING_SIZE': [128,128,128, 256,256,256, 384,384,384],\n",
    "    'WINDOW_SIZE' : [3,4,5, 3,4,5, 3,4,5],\n",
    "    #'EPOCHS': [20, 30, 40,  20, 30, 40,  20, 30, 40]\n",
    "    'EPOCHS': [50] * MODEL_COUNT\n",
    "}\n",
    "\n",
    "# parameter 잘못 넣었는지 검증\n",
    "assert len(W2V_TRAIN_PARAMS['MODEL_NAME']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['MAX_VOCAB_SIZE']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['EMBEDDING_SIZE']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['WINDOW_SIZE']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['EPOCHS']) == MODEL_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_count만큼 loop돌며 n개의 embedding training.\n",
    "\n",
    "## picked_model_index 지정하면 해당 index에 해당되는 parameter를 읽어 하나의 embedding 생성(테스트용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_w2v_model(picked_model_index, params):\n",
    "    for i, (model_name, max_vocab_size, embedding_size, window_size, epochs) in \\\n",
    "            enumerate(zip(params['MODEL_NAME'],\n",
    "                          params['MAX_VOCAB_SIZE'],\n",
    "                          params['EMBEDDING_SIZE'],\n",
    "                          params['WINDOW_SIZE'],\n",
    "                          params['EPOCHS'])):\n",
    "        if picked_model_index == -1:\n",
    "            pass\n",
    "        elif picked_model_index != i:\n",
    "            continue\n",
    "        \n",
    "        print(f'---- {i} 시작!! ----')\n",
    "        w2v_model = wv.Word2VecModel()\n",
    "        w2v_model.create(corpora_file_name, \n",
    "                           w2v_model_file_name_prefix + model_name, \n",
    "                           max_vocab_size=max_vocab_size, \n",
    "                           embedding_size=embedding_size,\n",
    "                           epochs=epochs,\n",
    "                           window=window_size,\n",
    "                           workers=WORKERS)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 시작!! ----\n",
      "10000개의 단어 내에서 최소 빈도수는 49입니다.\n",
      "Epoch: 1\tLoss after epoch 1: current loss : 17138032.0, previous loss : 0, diff : 17138032.0 \n",
      "Epoch: 2\tLoss after epoch 2: current loss : 30356626.0, previous loss : 17138032.0, diff : 13218594.0 \n",
      "Epoch: 3\tLoss after epoch 3: current loss : 40177552.0, previous loss : 30356626.0, diff : 9820926.0 \n",
      "Epoch: 4\tLoss after epoch 4: current loss : 48855448.0, previous loss : 40177552.0, diff : 8677896.0 \n",
      "Epoch: 5\tLoss after epoch 5: current loss : 57476980.0, previous loss : 48855448.0, diff : 8621532.0 \n",
      "Epoch: 6\tLoss after epoch 6: current loss : 66050640.0, previous loss : 57476980.0, diff : 8573660.0 \n",
      "Epoch: 7\tLoss after epoch 7: current loss : 68428624.0, previous loss : 66050640.0, diff : 2377984.0 \n",
      "Epoch: 8\tLoss after epoch 8: current loss : 69950336.0, previous loss : 68428624.0, diff : 1521712.0 \n",
      "Epoch: 9\tLoss after epoch 9: current loss : 71434176.0, previous loss : 69950336.0, diff : 1483840.0 \n",
      "Epoch: 10\tLoss after epoch 10: current loss : 72922288.0, previous loss : 71434176.0, diff : 1488112.0 \n",
      "Epoch: 11\tLoss after epoch 11: current loss : 74394760.0, previous loss : 72922288.0, diff : 1472472.0 \n",
      "Epoch: 12\tLoss after epoch 12: current loss : 75865848.0, previous loss : 74394760.0, diff : 1471088.0 \n",
      "Epoch: 13\tLoss after epoch 13: current loss : 77319440.0, previous loss : 75865848.0, diff : 1453592.0 \n",
      "Epoch: 14\tLoss after epoch 14: current loss : 78773640.0, previous loss : 77319440.0, diff : 1454200.0 \n",
      "Epoch: 15\tLoss after epoch 15: current loss : 80206696.0, previous loss : 78773640.0, diff : 1433056.0 \n",
      "Epoch: 16\tLoss after epoch 16: current loss : 81642680.0, previous loss : 80206696.0, diff : 1435984.0 \n",
      "Epoch: 17\tLoss after epoch 17: current loss : 83074272.0, previous loss : 81642680.0, diff : 1431592.0 \n",
      "Epoch: 18\tLoss after epoch 18: current loss : 84487496.0, previous loss : 83074272.0, diff : 1413224.0 \n",
      "Epoch: 19\tLoss after epoch 19: current loss : 85890728.0, previous loss : 84487496.0, diff : 1403232.0 \n",
      "Epoch: 20\tLoss after epoch 20: current loss : 87302112.0, previous loss : 85890728.0, diff : 1411384.0 \n",
      "Epoch: 21\tLoss after epoch 21: current loss : 88689328.0, previous loss : 87302112.0, diff : 1387216.0 \n",
      "Epoch: 22\tLoss after epoch 22: current loss : 90075240.0, previous loss : 88689328.0, diff : 1385912.0 \n",
      "Epoch: 23\tLoss after epoch 23: current loss : 91448560.0, previous loss : 90075240.0, diff : 1373320.0 \n",
      "Epoch: 24\tLoss after epoch 24: current loss : 92826480.0, previous loss : 91448560.0, diff : 1377920.0 \n",
      "Epoch: 25\tLoss after epoch 25: current loss : 94202976.0, previous loss : 92826480.0, diff : 1376496.0 \n",
      "Epoch: 26\tLoss after epoch 26: current loss : 95561704.0, previous loss : 94202976.0, diff : 1358728.0 \n",
      "Epoch: 27\tLoss after epoch 27: current loss : 96907552.0, previous loss : 95561704.0, diff : 1345848.0 \n",
      "Epoch: 28\tLoss after epoch 28: current loss : 98259856.0, previous loss : 96907552.0, diff : 1352304.0 \n",
      "Epoch: 29\tLoss after epoch 29: current loss : 99615160.0, previous loss : 98259856.0, diff : 1355304.0 \n",
      "Epoch: 30\tLoss after epoch 30: current loss : 100949032.0, previous loss : 99615160.0, diff : 1333872.0 \n",
      "Epoch: 31\tLoss after epoch 31: current loss : 102265360.0, previous loss : 100949032.0, diff : 1316328.0 \n",
      "Epoch: 32\tLoss after epoch 32: current loss : 103590888.0, previous loss : 102265360.0, diff : 1325528.0 \n",
      "Epoch: 33\tLoss after epoch 33: current loss : 104901552.0, previous loss : 103590888.0, diff : 1310664.0 \n",
      "Epoch: 34\tLoss after epoch 34: current loss : 106186960.0, previous loss : 104901552.0, diff : 1285408.0 \n",
      "Epoch: 35\tLoss after epoch 35: current loss : 107482736.0, previous loss : 106186960.0, diff : 1295776.0 \n",
      "Epoch: 36\tLoss after epoch 36: current loss : 108774904.0, previous loss : 107482736.0, diff : 1292168.0 \n",
      "Epoch: 37\tLoss after epoch 37: current loss : 110051568.0, previous loss : 108774904.0, diff : 1276664.0 \n",
      "Epoch: 38\tLoss after epoch 38: current loss : 111327528.0, previous loss : 110051568.0, diff : 1275960.0 \n",
      "Epoch: 39\tLoss after epoch 39: current loss : 112603560.0, previous loss : 111327528.0, diff : 1276032.0 \n",
      "Epoch: 40\tLoss after epoch 40: current loss : 113858184.0, previous loss : 112603560.0, diff : 1254624.0 \n",
      "Epoch: 41\tLoss after epoch 41: current loss : 115120768.0, previous loss : 113858184.0, diff : 1262584.0 \n",
      "Epoch: 42\tLoss after epoch 42: current loss : 116369648.0, previous loss : 115120768.0, diff : 1248880.0 \n",
      "Epoch: 43\tLoss after epoch 43: current loss : 117609072.0, previous loss : 116369648.0, diff : 1239424.0 \n",
      "Epoch: 44\tLoss after epoch 44: current loss : 118831648.0, previous loss : 117609072.0, diff : 1222576.0 \n",
      "Epoch: 45\tLoss after epoch 45: current loss : 120052584.0, previous loss : 118831648.0, diff : 1220936.0 \n",
      "Epoch: 46\tLoss after epoch 46: current loss : 121266792.0, previous loss : 120052584.0, diff : 1214208.0 \n",
      "Epoch: 47\tLoss after epoch 47: current loss : 122481384.0, previous loss : 121266792.0, diff : 1214592.0 \n",
      "Epoch: 48\tLoss after epoch 48: current loss : 123689656.0, previous loss : 122481384.0, diff : 1208272.0 \n",
      "Epoch: 49\tLoss after epoch 49: current loss : 124880984.0, previous loss : 123689656.0, diff : 1191328.0 \n",
      "Epoch: 50\tLoss after epoch 50: current loss : 126059776.0, previous loss : 124880984.0, diff : 1178792.0 \n",
      "---- 1 시작!! ----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ead2ac080eea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 테스트로 하나만 만들자.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mndx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcreate_multi_w2v_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2V_TRAIN_PARAMS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 모든 파라메터에 대해 만드려면 picked_model_index를 -1로\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-2f2655bfb722>\u001b[0m in \u001b[0;36mcreate_multi_w2v_model\u001b[1;34m(picked_model_index, params)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'---- {i} 시작!! ----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mw2v_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2VecModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         w2v_model.create(corpora_file_name, \n\u001b[0m\u001b[0;32m     16\u001b[0m                            \u001b[0mw2v_model_file_name_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                            \u001b[0mmax_vocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_vocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\TCL2021_telco_embedding\\Word2VecModel.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, in_corpus_file_name, out_model_file_name, max_vocab_size, embedding_size, epochs, window, workers)\u001b[0m\n\u001b[0;32m     29\u001b[0m                   for sentence in open(in_corpus_file_name, 'r', encoding='utf-8').readlines()]\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# 빈도수 상위 n위의 최소빈도수 구함 (word2vec 훈련 시 그 이하는 버리기 위함)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mmin_freq_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_min_freq_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_vocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{max_vocab_size}개의 단어 내에서 최소 빈도수는 {min_freq_cnt}입니다.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\TCL2021_telco_embedding\\Word2VecModel.py\u001b[0m in \u001b[0;36m_get_min_freq_count\u001b[1;34m(self, sentences, max_freq_rank)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# 빈도수 상위 vocab_size 내에 존재하는 단어 중 최소 빈도수를 구함\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_min_freq_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_freq_rank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mfdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_freq_rank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# the count of the the top-kth word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mCounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;31m# Cached number of samples in this FreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    550\u001b[0m         '''\n\u001b[0;32m    551\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    635\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mOverride\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0minvalidate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcached\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \"\"\"\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 테스트로 하나만 만들자.\n",
    "ndx = -1\n",
    "create_multi_w2v_model(ndx, W2V_TRAIN_PARAMS)\n",
    "\n",
    "# 모든 파라메터에 대해 만드려면 picked_model_index를 -1로\n",
    "#create_multi_w2v_model(-1, W2V_TRAIN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트로 0번째 모델 한번 가져와보자.\n",
    "w2v_model = wv.Word2VecModel()\n",
    "w2v_model.load(w2v_model_file_name_prefix + W2V_TRAIN_PARAMS['MODEL_NAME'][ndx])\n",
    "\n",
    "print( len(w2v_model.index2word) )\n",
    "print( len(w2v_model.word2index) )\n",
    "print( len(w2v_model.weight) )\n",
    "\n",
    "print( w2v_model.index2word[200] )\n",
    "print( w2v_model.word2index['약정'] )\n",
    "print( w2v_model.weight[2583] )\n",
    "print( w2v_model.norm_weight[2583] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이 밑은 fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_fasttext_model(picked_model_index, params):\n",
    "    for i, (model_name, max_vocab_size, embedding_size, window_size, epochs) in \\\n",
    "            enumerate(zip(params['MODEL_NAME'],\n",
    "                          params['MAX_VOCAB_SIZE'],\n",
    "                          params['EMBEDDING_SIZE'],\n",
    "                          params['WINDOW_SIZE'],\n",
    "                          params['EPOCHS'])):\n",
    "        if picked_model_index != i:\n",
    "            continue        \n",
    "        \n",
    "        print(f'---- {i} 시작!! ----')\n",
    "        ft_model = ft.FastTextModel()\n",
    "        ft_model.create(corpora_file_name, \n",
    "                           ft_model_file_name_prefix + model_name, \n",
    "                           max_vocab_size=max_vocab_size, \n",
    "                           embedding_size=embedding_size,\n",
    "                           epochs=epochs, \n",
    "                           window=window_size,\n",
    "                           workers=WORKERS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 8 시작!! ----\n",
      "10000개의 단어 내에서 최소 빈도수는 49입니다.\n",
      "Epoch: 1\tLoss after epoch 1: current loss : 0.0, previous loss : 0, diff : 0.0 \n"
     ]
    }
   ],
   "source": [
    "# 테스트로 하나만 만들자.\n",
    "ndx = 8\n",
    "create_multi_fasttext_model(ndx, W2V_TRAIN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트로 ndx번째 모델 한번 가져와보자. - fasttext\n",
    "w2v_model = ft.FastTextModel()\n",
    "w2v_model.load(ft_model_file_name_prefix + W2V_TRAIN_PARAMS['MODEL_NAME'][ndx])\n",
    "\n",
    "print( len(w2v_model.index2word) )\n",
    "print( len(w2v_model.word2index) )\n",
    "print( len(w2v_model.weight) )\n",
    "\n",
    "print( w2v_model.index2word[200] )\n",
    "print( w2v_model.word2index['약정'] )\n",
    "print( w2v_model.weight[2583] )\n",
    "print( w2v_model.norm_weight[2583] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
