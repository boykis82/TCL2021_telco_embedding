{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boyki\\anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# my lib\n",
    "import textlib as tl\n",
    "import Word2VecModel as wv\n",
    "import FastTextModel as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 병렬 수행을 위한 core 개수 가져오기 (모든 코어를 사용하면 다른 작업이 불가하여 -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKERS = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 사용 가능 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 말뭉치 경로\n",
    "corpora_prttag_file_name = '../TCL2021_Telco_Embedding_Dataset/corpora/telco_corpora.dat'\n",
    "corpora_alltag_file_name = '../TCL2021_Telco_Embedding_Dataset/corpora/telco_corpora_all_tag.dat'\n",
    "\n",
    "# 일부 tag(명사, 형용사 계열)로만 만든 embedding vector를 저장할 경로\n",
    "w2v_model_prttag_file_name_prefix = '../TCL2021_Telco_Embedding_Dataset/embedding_w2v/telco_w2v_'\n",
    "\n",
    "# 모든 tag로 만든 embedding vector를 저장할 경로\n",
    "w2v_model_alltag_file_name_prefix = '../TCL2021_Telco_Embedding_Dataset/embedding_w2v_alltag/telco_w2v_'\n",
    "\n",
    "# 모든 tag로 만든 fasttext embedding vector를 저장할 경로\n",
    "ft_model_file_name_prefix = '../TCL2021_Telco_Embedding_Dataset/embedding_fasttext/telco_ft_'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일부 형태소 사용할지, 전체 형태소 사용할지 여부에 따라 아래 코드 변경하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v 모든 형태소 사용\n",
    "#w2v_model_file_name_prefix = w2v_model_alltag_file_name_prefix\n",
    "#corpora_file_name = corpora_alltag_file_name\n",
    "\n",
    "# w2v 일부 형태소 사용\n",
    "#w2v_model_file_name_prefix = w2v_model_prttag_file_name_prefix\n",
    "#corpora_file_name = corpora_prttag_file_name\n",
    "\n",
    "# fasttext\n",
    "w2v_model_file_name_prefix = ft_model_file_name_prefix\n",
    "corpora_file_name = corpora_alltag_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 말뭉치 통계 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sentence.strip().split(' ') \n",
    "                  for sentence in open(corpora_file_name, 'r', encoding='utf-8').readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from itertools import chain\n",
    "fdist = nltk.FreqDist(chain.from_iterable(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000, 40000, 1000):\n",
    "    print( str(i) + '------>' + str(fdist.most_common(i)[-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어수는 10,000개로 고정. embedding 차원수 [128,256,384], window size [3,4,5]의 조합으로 총 9개 embedding 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 여러개의 w2v 모델을 만들기 위한 table\n",
    "MODEL_COUNT = 9\n",
    "\n",
    "W2V_TRAIN_PARAMS = {\n",
    "    'MODEL_NAME': ['V10000_E128_W3','V10000_E128_W4','V10000_E128_W5',\n",
    "                   'V10000_E256_W3','V10000_E256_W4','V10000_E256_W5',\n",
    "                   'V10000_E384_W3','V10000_E384_W4','V10000_E384_W5'],\n",
    "    'MAX_VOCAB_SIZE': [10000] * MODEL_COUNT,\n",
    "    'EMBEDDING_SIZE': [128,128,128, 256,256,256, 384,384,384],\n",
    "    'WINDOW_SIZE' : [3,4,5, 3,4,5, 3,4,5],\n",
    "    #'EPOCHS': [20, 30, 40,  20, 30, 40,  20, 30, 40]\n",
    "    'EPOCHS': [50] * MODEL_COUNT\n",
    "}\n",
    "\n",
    "# parameter 잘못 넣었는지 검증\n",
    "assert len(W2V_TRAIN_PARAMS['MODEL_NAME']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['MAX_VOCAB_SIZE']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['EMBEDDING_SIZE']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['WINDOW_SIZE']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['EPOCHS']) == MODEL_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_count만큼 loop돌며 n개의 embedding training.\n",
    "\n",
    "## picked_model_index 지정하면 해당 index에 해당되는 parameter를 읽어 하나의 embedding 생성(테스트용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_w2v_model(picked_model_index, params):\n",
    "    for i, (model_name, max_vocab_size, embedding_size, window_size, epochs) in \\\n",
    "            enumerate(zip(params['MODEL_NAME'],\n",
    "                          params['MAX_VOCAB_SIZE'],\n",
    "                          params['EMBEDDING_SIZE'],\n",
    "                          params['WINDOW_SIZE'],\n",
    "                          params['EPOCHS'])):\n",
    "        if picked_model_index != i:\n",
    "            continue\n",
    "        \n",
    "        print(f'---- {i} 시작!! ----')\n",
    "        w2v_model = wv.Word2VecModel()\n",
    "        w2v_model.create(corpora_file_name, \n",
    "                           w2v_model_file_name_prefix + model_name, \n",
    "                           max_vocab_size=max_vocab_size, \n",
    "                           embedding_size=embedding_size,\n",
    "                           epochs=epochs,\n",
    "                           window=window_size,\n",
    "                           workers=WORKERS)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 1 시작!! ----\n",
      "10000개의 단어 내에서 최소 빈도수는 49입니다.\n",
      "Epoch: 1\tLoss after epoch 1: current loss : 20664828.0, previous loss : 0, diff : 20664828.0 \n",
      "Epoch: 2\tLoss after epoch 2: current loss : 36077252.0, previous loss : 20664828.0, diff : 15412424.0 \n",
      "Epoch: 3\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d8071679baec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 테스트로 하나만 만들자.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mndx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcreate_multi_w2v_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2V_TRAIN_PARAMS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 모든 파라메터에 대해 만드려면 picked_model_index를 -1로\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-52252ffc5a77>\u001b[0m in \u001b[0;36mcreate_multi_w2v_model\u001b[1;34m(picked_model_index, params)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'---- {i} 시작!! ----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mw2v_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2VecModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         w2v_model.create(corpora_file_name, \n\u001b[0m\u001b[0;32m     14\u001b[0m                            \u001b[0mw2v_model_file_name_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                            \u001b[0mmax_vocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_vocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\TCL2021_telco_embedding\\Word2VecModel.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, in_corpus_file_name, out_model_file_name, max_vocab_size, embedding_size, epochs, window, workers)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mloss_logger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLossLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# gensim word2vec call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         self.gensim_w2v_model = Word2Vec(corpus, \n\u001b[0m\u001b[0;32m     37\u001b[0m                          \u001b[1;31m#size=embedding_size,    # 윈도우에서는 size, linux에서는 embedding_size... 버전 문제인가?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                          \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab)\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             self.train(\n\u001b[0m\u001b[0;32m    420\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[0;32m   1063\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1421\u001b[0m             \u001b[0mthread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1423\u001b[1;33m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[0;32m   1424\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 테스트로 하나만 만들자.\n",
    "ndx = 1\n",
    "create_multi_w2v_model(ndx, W2V_TRAIN_PARAMS)\n",
    "\n",
    "# 모든 파라메터에 대해 만드려면 picked_model_index를 -1로\n",
    "#create_multi_w2v_model(-1, W2V_TRAIN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트로 0번째 모델 한번 가져와보자.\n",
    "w2v_model = wv.Word2VecModel()\n",
    "w2v_model.load(w2v_model_file_name_prefix + W2V_TRAIN_PARAMS['MODEL_NAME'][ndx])\n",
    "\n",
    "print( len(w2v_model.index2word) )\n",
    "print( len(w2v_model.word2index) )\n",
    "print( len(w2v_model.weight) )\n",
    "\n",
    "print( w2v_model.index2word[200] )\n",
    "print( w2v_model.word2index['약정'] )\n",
    "print( w2v_model.weight[2583] )\n",
    "print( w2v_model.norm_weight[2583] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이 밑은 fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_fasttext_model(picked_model_index, params):\n",
    "    for i, (model_name, max_vocab_size, embedding_size, window_size, epochs) in \\\n",
    "            enumerate(zip(params['MODEL_NAME'],\n",
    "                          params['MAX_VOCAB_SIZE'],\n",
    "                          params['EMBEDDING_SIZE'],\n",
    "                          params['WINDOW_SIZE'],\n",
    "                          params['EPOCHS'])):\n",
    "        if picked_model_index != i:\n",
    "            continue        \n",
    "        \n",
    "        print(f'---- {i} 시작!! ----')\n",
    "        ft_model = ft.FastTextModel()\n",
    "        ft_model.create(corpora_file_name, \n",
    "                           ft_model_file_name_prefix + model_name, \n",
    "                           max_vocab_size=max_vocab_size, \n",
    "                           embedding_size=embedding_size,\n",
    "                           epochs=epochs, \n",
    "                           window=window_size,\n",
    "                           workers=WORKERS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 8 시작!! ----\n",
      "10000개의 단어 내에서 최소 빈도수는 49입니다.\n",
      "Epoch: 1\tLoss after epoch 1: current loss : 0.0, previous loss : 0, diff : 0.0 \n"
     ]
    }
   ],
   "source": [
    "# 테스트로 하나만 만들자.\n",
    "ndx = 8\n",
    "create_multi_fasttext_model(ndx, W2V_TRAIN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트로 ndx번째 모델 한번 가져와보자. - fasttext\n",
    "w2v_model = ft.FastTextModel()\n",
    "w2v_model.load(ft_model_file_name_prefix + W2V_TRAIN_PARAMS['MODEL_NAME'][ndx])\n",
    "\n",
    "print( len(w2v_model.index2word) )\n",
    "print( len(w2v_model.word2index) )\n",
    "print( len(w2v_model.weight) )\n",
    "\n",
    "print( w2v_model.index2word[200] )\n",
    "print( w2v_model.word2index['약정'] )\n",
    "print( w2v_model.weight[2583] )\n",
    "print( w2v_model.norm_weight[2583] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
