{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "# my lib\n",
    "import textlib as tl\n",
    "import Word2VecModel as wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKERS = multiprocessing.cpu_count() - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input / output file name\n",
    "corpora_file_name = 'D:/ml/TCL2021_telco_embedding/dataset/telco_corpora.dat'\n",
    "w2v_model_file_name_prefix = 'D:/ml/TCL2021_telco_embedding/dataset/w2v/telco_w2v_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 여러개의 w2v 모델을 만들기 위한 table\n",
    "MODEL_COUNT = 9\n",
    "W2V_TRAIN_PARAMS = {\n",
    "    'MODEL_NAME': ['V10000_E100_W3_E50','V10000_E100_W5_E50','V10000_E100_W7_E50',\n",
    "                   'V10000_E200_W3_E50','V10000_E200_W5_E50','V10000_E200_W7_E50',\n",
    "                   'V10000_E300_W3_E50','V10000_E300_W5_E50','V10000_E300_W7_E50'],\n",
    "    'MAX_VOCAB_SIZE': [10000] * MODEL_COUNT,\n",
    "    'EMBEDDING_SIZE': [100,100,100,200,200,200,300,300,300],\n",
    "    'WINDOW_SIZE' : [3,5,7,3,5,7,3,5,7],\n",
    "    'EPOCHS': [50] * MODEL_COUNT\n",
    "}\n",
    "\n",
    "# parameter 잘못 넣었는지 검증\n",
    "assert len(W2V_TRAIN_PARAMS['MODEL_NAME']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['MAX_VOCAB_SIZE']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['EMBEDDING_SIZE']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['WINDOW_SIZE']) == MODEL_COUNT\n",
    "assert len(W2V_TRAIN_PARAMS['EPOCHS']) == MODEL_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_w2v_model(model_count, params):\n",
    "    for i, (model_name, max_vocab_size, embedding_size, window_size, epochs) in \\\n",
    "            enumerate(zip(params['MODEL_NAME'],\n",
    "                          params['MAX_VOCAB_SIZE'],\n",
    "                          params['EMBEDDING_SIZE'],\n",
    "                          params['WINDOW_SIZE'],\n",
    "                          params['EPOCHS'])):\n",
    "        print(f'---- {i} 시작!! ----')\n",
    "        w2v_model = wv.Word2VecModel()\n",
    "        w2v_model.create(corpora_file_name, \n",
    "                           w2v_model_file_name_prefix + model_name, \n",
    "                           max_vocab_size=max_vocab_size, \n",
    "                           embedding_size=embedding_size,\n",
    "                           epochs=epochs,\n",
    "                           window=window_size,\n",
    "                           workers=WORKERS)    \n",
    "        \n",
    "        if i + 1 >= model_count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 시작!! ----\n",
      "10000개의 단어 내에서 최소 빈도수는 43입니다.\n",
      "Epoch: 1\tLoss after epoch 1: 12431698.0\n",
      "Epoch: 2\tLoss after epoch 2: 9763266.0\n",
      "Epoch: 3\tLoss after epoch 3: 9019944.0\n",
      "Epoch: 4\tLoss after epoch 4: 6721448.0\n",
      "Epoch: 5\tLoss after epoch 5: 5902464.0\n",
      "Epoch: 6\tLoss after epoch 6: 5894952.0\n",
      "Epoch: 7\tLoss after epoch 7: 5884408.0\n",
      "Epoch: 8\tLoss after epoch 8: 5865208.0\n",
      "Epoch: 9\tLoss after epoch 9: 5667524.0\n",
      "Epoch: 10\tLoss after epoch 10: 1087136.0\n",
      "Epoch: 11\tLoss after epoch 11: 1092072.0\n",
      "Epoch: 12\tLoss after epoch 12: 1091512.0\n",
      "Epoch: 13\tLoss after epoch 13: 1077552.0\n",
      "Epoch: 14\tLoss after epoch 14: 1076736.0\n",
      "Epoch: 15\tLoss after epoch 15: 1060232.0\n",
      "Epoch: 16\tLoss after epoch 16: 1067376.0\n",
      "Epoch: 17\tLoss after epoch 17: 1053688.0\n",
      "Epoch: 18\tLoss after epoch 18: 1060096.0\n",
      "Epoch: 19\tLoss after epoch 19: 1056888.0\n",
      "Epoch: 20\tLoss after epoch 20: 1057376.0\n",
      "Epoch: 21\tLoss after epoch 21: 1023216.0\n",
      "Epoch: 22\tLoss after epoch 22: 1037632.0\n",
      "Epoch: 23\tLoss after epoch 23: 1022240.0\n",
      "Epoch: 24\tLoss after epoch 24: 1008072.0\n",
      "Epoch: 25\tLoss after epoch 25: 1005864.0\n",
      "Epoch: 26\tLoss after epoch 26: 995960.0\n",
      "Epoch: 27\tLoss after epoch 27: 1005248.0\n",
      "Epoch: 28\tLoss after epoch 28: 996128.0\n",
      "Epoch: 29\tLoss after epoch 29: 986976.0\n",
      "Epoch: 30\tLoss after epoch 30: 987680.0\n",
      "Epoch: 31\tLoss after epoch 31: 986352.0\n",
      "Epoch: 32\tLoss after epoch 32: 988376.0\n",
      "Epoch: 33\tLoss after epoch 33: 971776.0\n",
      "Epoch: 34\tLoss after epoch 34: 959288.0\n",
      "Epoch: 35\tLoss after epoch 35: 963176.0\n",
      "Epoch: 36\tLoss after epoch 36: 945680.0\n",
      "Epoch: 37\tLoss after epoch 37: 946760.0\n",
      "Epoch: 38\tLoss after epoch 38: 942096.0\n",
      "Epoch: 39\tLoss after epoch 39: 939384.0\n",
      "Epoch: 40\tLoss after epoch 40: 923824.0\n",
      "Epoch: 41\tLoss after epoch 41: 928520.0\n",
      "Epoch: 42\tLoss after epoch 42: 920720.0\n",
      "Epoch: 43\tLoss after epoch 43: 921560.0\n",
      "Epoch: 44\tLoss after epoch 44: 909992.0\n",
      "Epoch: 45\tLoss after epoch 45: 917360.0\n",
      "Epoch: 46\tLoss after epoch 46: 901144.0\n",
      "Epoch: 47\tLoss after epoch 47: 897120.0\n",
      "Epoch: 48\tLoss after epoch 48: 899944.0\n",
      "Epoch: 49\tLoss after epoch 49: 902704.0\n",
      "Epoch: 50\tLoss after epoch 50: 895752.0\n"
     ]
    }
   ],
   "source": [
    "# 테스트로 하나만 만들자.\n",
    "model_create_count = 1\n",
    "create_multi_w2v_model(model_create_count, W2V_TRAIN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10086\n",
      "10086\n",
      "10086\n",
      "채널\n",
      "83\n",
      "[ 0.33492824 -0.14252254  0.30957681 -0.10566582  0.25233808 -0.3331652\n",
      "  0.05035937  0.23592699  0.58263183 -0.29077598  0.28332579 -0.21423478\n",
      "  0.41137639  0.22636791 -0.38939717  0.49806896 -0.38530859 -0.03138449\n",
      "  0.28518784  0.44729728  0.27533096 -0.51609653  0.43539822  0.17607941\n",
      "  0.57308578 -0.12231185  0.5415228  -0.03349705 -0.16551206  0.36280257\n",
      "  0.44494292  0.27118194  0.82575399 -0.14198819  0.65345013 -0.12884913\n",
      " -0.51451284  0.239757   -0.1925742   0.43418971  0.03470463  0.50680977\n",
      " -0.24023718 -0.23700158  0.02458469  0.14715017  0.35727835 -0.38482237\n",
      "  0.03579685  0.00836114  0.2008189  -0.05320696 -0.27793506 -0.24184163\n",
      "  1.11740804  0.74650443 -0.51975143  0.157313   -0.02495085  0.26140296\n",
      "  0.61399764  0.05733346 -0.23632856 -0.167316   -0.29453295 -0.47333759\n",
      "  0.31321493 -0.41636497  0.10765453 -0.05395645  0.32566011 -0.10642837\n",
      "  0.05548593 -0.74542451 -0.38708597  0.1759312  -0.49640876 -0.00934392\n",
      "  0.58048266  0.66551787  0.63235438 -0.04341833  0.32185888  0.21585731\n",
      " -0.12744749  0.31614879  0.25195345  0.39312834  0.79806781 -0.70153242\n",
      "  1.02095234  0.28294313  0.12403885 -0.62052751 -0.51326948  0.39105085\n",
      " -0.30730751  0.38319346  0.5284211   0.42670104]\n",
      "[ 0.08368205 -0.03560935  0.07734797 -0.02640068  0.06304684 -0.08324155\n",
      "  0.01258232  0.05894652  0.14557095 -0.07265058  0.07078914 -0.05352671\n",
      "  0.10278267  0.05655817 -0.09729115  0.12444286 -0.09626961 -0.00784144\n",
      "  0.07125437  0.11175753  0.06879163 -0.12894707  0.10878454  0.04399356\n",
      "  0.14318587 -0.0305597   0.13529984 -0.00836926 -0.0413533   0.09064647\n",
      "  0.11116929  0.06775499  0.20631519 -0.03547584  0.16326496 -0.03219304\n",
      " -0.12855138  0.05990345 -0.04811479  0.10848259  0.00867097  0.12662676\n",
      " -0.06002342 -0.05921501  0.0061425   0.03676557  0.08926624 -0.09614813\n",
      "  0.00894387  0.00208904  0.05017474 -0.01329379 -0.06944226 -0.06042429\n",
      "  0.27918515  0.18651463 -0.12986024  0.03930476 -0.00623399  0.06531171\n",
      "  0.15340772  0.0143248  -0.05904685 -0.04180402 -0.07358926 -0.11826372\n",
      "  0.07825696 -0.10402907  0.02689756 -0.01348106  0.0813664  -0.0265912\n",
      "  0.0138632  -0.18624482 -0.09671369  0.04395653 -0.12402806 -0.00233459\n",
      "  0.14503398  0.16628009  0.15799417 -0.0108481   0.08041666  0.05393209\n",
      " -0.03184284  0.07898999  0.06295074  0.09822338  0.19939778 -0.17527835\n",
      "  0.25508563  0.07069353  0.03099119 -0.15503922 -0.12824073  0.09770432\n",
      " -0.07678099  0.09574114  0.13202637  0.10661154]\n"
     ]
    }
   ],
   "source": [
    "# 테스트로 0번째 모델 한번 가져와보자.\n",
    "i = 0\n",
    "w2v_model = wv.Word2VecModel()\n",
    "w2v_model.load(w2v_model_file_name_prefix + W2V_TRAIN_PARAMS['MODEL_NAME'][i])\n",
    "\n",
    "print( len(w2v_model.index2word) )\n",
    "print( len(w2v_model.word2index) )\n",
    "print( len(w2v_model.weight) )\n",
    "\n",
    "print( w2v_model.index2word[200] )\n",
    "print( w2v_model.word2index['약정'] )\n",
    "print( w2v_model.weight[2583] )\n",
    "print( w2v_model.norm_weight[2583] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
