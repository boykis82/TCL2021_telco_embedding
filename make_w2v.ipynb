{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "international-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import multiprocessing\n",
    "from eunjeon import Mecab\n",
    "import eunjeon\n",
    "import konlpy\n",
    "import textlib as tl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "organized-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_SIZE = 100\n",
    "WORKERS = multiprocessing.cpu_count() - 1\n",
    "WINDOW_SIZE = 5\n",
    "EPOCHS = 10\n",
    "\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD_TOKEN = '<PAD>'\n",
    "tokenizer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "southwest-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_w2v_model(in_file_name, out_file_name,\n",
    "                   max_vocab_size=10000, embedding_size=100,\n",
    "                   epochs=10, window=5, workers=3):\n",
    "    # 빈도수 상위 vocab_size 내에 존재하는 단어 중 최소 빈도수를 구함\n",
    "    def get_min_freq_count(sentences, max_freq_rank):\n",
    "        from itertools import chain\n",
    "        import nltk\n",
    "\n",
    "        fdist = nltk.FreqDist(chain.from_iterable(sentences))\n",
    "        return fdist.most_common(max_freq_rank)[-1][1] # the count of the the top-kth word\n",
    "\n",
    "    # 단어 모음\n",
    "    corpus = [sentence.strip().split(' ') \n",
    "              for sentence in open(in_file_name, 'r', encoding='utf-8').readlines()]\n",
    "    # 빈도수 상위 n위의 최소빈도수 구함 (word2vec 훈련 시 그 이하는 버리기 위함)\n",
    "    min_freq_cnt = get_min_freq_count(corpus, max_vocab_size)\n",
    "    print(f'{max_vocab_size}개의 단어 내에서 최소 빈도수는 {min_freq_cnt}입니다.')\n",
    "    \n",
    "    # gensim word2vec call\n",
    "    w2v_model = Word2Vec(corpus, \n",
    "                     size=embedding_size, \n",
    "                     workers=workers, \n",
    "                     min_count=min_freq_cnt,\n",
    "                     sg=1, \n",
    "                     iter=epochs,\n",
    "                     window=window)\n",
    "    # 저장\n",
    "    w2v_model.save(out_file_name)        \n",
    "    \n",
    "    return _post_w2v_process(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adjustable-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _post_w2v_process(w2v_model):    \n",
    "    # unknown, padding 토큰 추가\n",
    "    def _append_unk_pad_vectors(embeddings):\n",
    "        embedding_size = embeddings.shape[1]\n",
    "        def get_truncated_normal(mean=0, sd=1, low=-1, upp=1):\n",
    "            from scipy.stats import truncnorm\n",
    "            return truncnorm(\n",
    "                    (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "        return np.append(embeddings, \n",
    "                         get_truncated_normal().rvs(embedding_size * 2).reshape(2, embedding_size), axis=0)    \n",
    "        \n",
    "    index2word = w2v_model.wv.index2word\n",
    "    # unk, pad 추가\n",
    "    index2word.append( UNK_TOKEN )\n",
    "    index2word.append( PAD_TOKEN )    \n",
    "    \n",
    "    w2v = w2v_model.wv.vectors\n",
    "    # unk, pad에 해당하는 normal 초기화된 벡터 추가\n",
    "    w2v = _append_unk_pad_vectors(w2v)\n",
    "    # cosine유사도 체크를 위해 normalize\n",
    "    unit_w2v = normalize(w2v, norm='l2', axis=1)\n",
    "\n",
    "    # word를 index로 변환\n",
    "    word2index = {w:i for i, w in enumerate(index2word)}\n",
    "    # 사전. word를 vector로 변환\n",
    "    dictionary = {w:v for w, v in zip(index2word, unit_w2v)}    \n",
    "    \n",
    "    return {\n",
    "        'w2v_model'   : w2v_model, \n",
    "        'index2word'  : index2word, \n",
    "        'word2index'  : word2index,\n",
    "\n",
    "        'weight'      : w2v,\n",
    "        'norm_weight' : unit_w2v\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sublime-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_file_name = 'D:/data/telco_corpora.dat'\n",
    "w2v_model_file_name = f'd:/data/telco_w2v_{MAX_VOCAB_SIZE}_{EMBEDDING_SIZE}_{EPOCHS}_{WINDOW_SIZE}.dat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# word2vec 모델 만들고 저장. 그 외 필요한 값들 리턴\n",
    "w2v_output = make_w2v_model(corpora_file_name, \n",
    "                   w2v_model_file_name, \n",
    "                   max_vocab_size=MAX_VOCAB_SIZE, \n",
    "                   embedding_size=EMBEDDING_SIZE,\n",
    "                   epochs=EPOCHS,\n",
    "                   window=WINDOW_SIZE,\n",
    "                   workers=WORKERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "lesser-disability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10200\n",
      "10200\n",
      "10200\n",
      "수납\n",
      "61\n",
      "[-0.35776106  0.21665499  0.12065426 -0.07808045 -0.08555077 -0.31346983\n",
      "  0.1906666   0.69993132 -0.27874622  0.33214924 -0.10627281 -0.43345928\n",
      "  0.64351821 -0.40301988 -0.10826819 -0.57930028  0.01342783 -0.02467114\n",
      " -0.00303033 -0.24342476  0.12172619 -0.22651088 -0.29879901  0.25398812\n",
      " -0.26312226  0.08500879 -0.76019847  0.23448333  0.18769121 -0.24128641\n",
      " -0.58256441  0.47644502 -0.20451702 -0.28703746 -0.4114756   0.25096068\n",
      " -0.45316589  0.57999337  0.12603375  0.18206236  0.46993253 -0.36381519\n",
      " -0.42706221 -0.14927141  0.35819951  0.60059774  0.48026413 -0.08949742\n",
      " -0.37397689 -0.1537744  -0.45392779  0.6441046   0.171067   -0.21600319\n",
      " -0.29178634  0.24486649 -0.94137931  0.14310238  0.06903156  0.15202139\n",
      " -0.04857434 -0.44160995  0.31435215 -0.14665219 -0.49364892  0.4428457\n",
      " -0.63872004 -0.76166147  0.13702716  0.20451474 -0.83049208  0.00682165\n",
      "  0.07786661 -0.4688977   0.11341438 -0.26523516  0.435202    0.15378253\n",
      " -0.21403897 -0.15495023 -0.69354397  0.85715431  0.67286271 -0.64946026\n",
      "  0.93077767 -0.8410669   0.06493738  0.21741483  0.25782418  0.5972572\n",
      " -0.56421071  0.38441753  0.46390674 -0.33960503  0.08782851 -0.10835318\n",
      " -0.13947619  0.41909921 -0.2406428  -0.23916245]\n",
      "[-0.0885769   0.05364091  0.0298724  -0.01933169 -0.02118124 -0.07761098\n",
      "  0.04720653  0.17329373 -0.06901388  0.08223576 -0.02631174 -0.10731878\n",
      "  0.15932659 -0.09978239 -0.02680577 -0.14342709  0.00332455 -0.00610825\n",
      " -0.00075027 -0.06026875  0.03013779 -0.0560811  -0.07397868  0.0628841\n",
      " -0.06514559  0.02104705 -0.18821508  0.05805497  0.04646986 -0.05973932\n",
      " -0.14423524  0.11796148 -0.05063571 -0.07106668 -0.10187591  0.06213454\n",
      " -0.11219788  0.14359868  0.03120429  0.04507623  0.11634908 -0.09007583\n",
      " -0.10573495 -0.03695763  0.08868546  0.14870005  0.11890704 -0.02215838\n",
      " -0.09259173 -0.03807251 -0.11238651  0.15947178  0.04235393 -0.05347953\n",
      " -0.07224244  0.0606257  -0.23307306  0.03543026  0.0170913   0.03763849\n",
      " -0.01202636 -0.10933678  0.07782943 -0.03630914 -0.12222094  0.10964274\n",
      " -0.15813863 -0.1885773   0.03392611  0.05063514 -0.20561885  0.00168895\n",
      "  0.01927874 -0.11609287  0.0280799  -0.06566872  0.10775026  0.03807452\n",
      " -0.05299322 -0.03836363 -0.17171231  0.21222007  0.1665919  -0.16079777\n",
      "  0.23044823 -0.20823703  0.01607763  0.05382903  0.06383386  0.14787298\n",
      " -0.13969111  0.09517669  0.11485717 -0.08408171  0.02174518 -0.02682681\n",
      " -0.03453246  0.10376342 -0.05957997 -0.05921346]\n"
     ]
    }
   ],
   "source": [
    "print( len(w2v_output['index2word']) )\n",
    "print( len(w2v_output['word2index']) )\n",
    "print( len(w2v_output['weight']) )\n",
    "\n",
    "print( w2v_output['index2word'][200] )\n",
    "print( w2v_output['word2index']['약정'] )\n",
    "print( w2v_output['weight'][2583] )\n",
    "print( w2v_output['norm_weight'][2583] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "posted-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    loss_ax = axs[0,0]\n",
    "    acc_ax = axs[0,1]\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "    acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "    acc_ax.set_ylabel('accuracy')\n",
    "    acc_ax.legend(loc='upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "physical-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 완료된 모델 있으면 로드해서 쓴다.\n",
    "def load_w2v_model(model_file_name):\n",
    "    w2v_model = Word2Vec.load(model_file_name)\n",
    "\n",
    "    return _post_w2v_process(w2v_model)\n",
    "\n",
    "w2v_output = load_w2v_model(w2v_model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "finished-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Layer, Dense, Embedding, Activation, LSTM, Bidirectional, GRU, Softmax, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "structural-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = 'd:/data/SOR/sor_dataset.xlsx'\n",
    "try:\n",
    "    df = pd.read_excel(input_file_name, sheet_name=0, engine='openpyxl')\n",
    "except FileNotFoundError:\n",
    "    print(f'{input_file_name}이 없습니다! skip!')\n",
    "\n",
    "# null 인 row가 하나라도 있으면 삭제\n",
    "df.dropna(axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "weighted-wages",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92483, 5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "friendly-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# co(요청회사)가 SKT, SKB인 것만 추출\n",
    "df = df[ ((df['co'] == 'SKT') | (df['co'] == 'SKB')) & \\\n",
    "             (\n",
    "                (df['label'] != 'Configuration') & \n",
    "                (df['label'] != 'EAI/EIGW') &\n",
    "                (df['label'] != 'I/F 유틸') &\n",
    "                (df['label'] != 'MTOKTOK') &\n",
    "                (df['label'] != 'PPS 상품권') &\n",
    "                (df['label'] != 'Utility') &\n",
    "                (df['label'] != '고객상담') &\n",
    "                (df['label'] != '접근 관리') &\n",
    "                (df['label'] != '코드 관리')\n",
    "             )\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "rough-southeast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92197, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "stone-campaign",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_ym</th>\n",
       "      <th>co</th>\n",
       "      <th>req_br</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Billing/OSS</th>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "      <td>6699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <td>6484</td>\n",
       "      <td>6484</td>\n",
       "      <td>6484</td>\n",
       "      <td>6484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRM</th>\n",
       "      <td>24176</td>\n",
       "      <td>24176</td>\n",
       "      <td>24176</td>\n",
       "      <td>24176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTC</th>\n",
       "      <td>1676</td>\n",
       "      <td>1676</td>\n",
       "      <td>1676</td>\n",
       "      <td>1676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer Care</th>\n",
       "      <td>11064</td>\n",
       "      <td>11064</td>\n",
       "      <td>11064</td>\n",
       "      <td>11064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERP</th>\n",
       "      <td>5661</td>\n",
       "      <td>5661</td>\n",
       "      <td>5661</td>\n",
       "      <td>5661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OCEAN</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSS</th>\n",
       "      <td>5458</td>\n",
       "      <td>5458</td>\n",
       "      <td>5458</td>\n",
       "      <td>5458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRM</th>\n",
       "      <td>2597</td>\n",
       "      <td>2597</td>\n",
       "      <td>2597</td>\n",
       "      <td>2597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TWORLD</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bizservice</th>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>경영정보/IT관리</th>\n",
       "      <td>489</td>\n",
       "      <td>489</td>\n",
       "      <td>489</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>공통</th>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>공통자원관리</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>그룹포탈</th>\n",
       "      <td>3764</td>\n",
       "      <td>3764</td>\n",
       "      <td>3764</td>\n",
       "      <td>3764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>기타</th>\n",
       "      <td>10013</td>\n",
       "      <td>10013</td>\n",
       "      <td>10013</td>\n",
       "      <td>10013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>무선OSS</th>\n",
       "      <td>1941</td>\n",
       "      <td>1941</td>\n",
       "      <td>1941</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>미납</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>빌링</th>\n",
       "      <td>2589</td>\n",
       "      <td>2589</td>\n",
       "      <td>2589</td>\n",
       "      <td>2589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>상품 및 오퍼 관리</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>수납관리</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>스마트플래너</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>연동</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>영업기타</th>\n",
       "      <td>5672</td>\n",
       "      <td>5672</td>\n",
       "      <td>5672</td>\n",
       "      <td>5672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>오더관리</th>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>인터페이스</th>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>인트라넷</th>\n",
       "      <td>741</td>\n",
       "      <td>741</td>\n",
       "      <td>741</td>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>자동납부</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>자원관리</th>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>재무/정산</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>청구관리</th>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>통계</th>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               req_ym     co  req_br  sentence\n",
       "label                                         \n",
       "Billing/OSS      6699   6699    6699      6699\n",
       "CC               6484   6484    6484      6484\n",
       "CRM             24176  24176   24176     24176\n",
       "CTC              1676   1676    1676      1676\n",
       "Customer Care   11064  11064   11064     11064\n",
       "ERP              5661   5661    5661      5661\n",
       "OCEAN              35     35      35        35\n",
       "OSS              5458   5458    5458      5458\n",
       "PRM              2597   2597    2597      2597\n",
       "TWORLD             83     83      83        83\n",
       "bizservice        145    145     145       145\n",
       "경영정보/IT관리         489    489     489       489\n",
       "공통               1308   1308    1308      1308\n",
       "공통자원관리             12     12      12        12\n",
       "그룹포탈             3764   3764    3764      3764\n",
       "기타              10013  10013   10013     10013\n",
       "무선OSS            1941   1941    1941      1941\n",
       "미납                 39     39      39        39\n",
       "빌링               2589   2589    2589      2589\n",
       "상품 및 오퍼 관리         43     43      43        43\n",
       "수납관리               99     99      99        99\n",
       "스마트플래너             17     17      17        17\n",
       "연동                545    545     545       545\n",
       "영업기타             5672   5672    5672      5672\n",
       "오더관리              258    258     258       258\n",
       "인터페이스             116    116     116       116\n",
       "인트라넷              741    741     741       741\n",
       "자동납부               32     32      32        32\n",
       "자원관리               86     86      86        86\n",
       "재무/정산              81     81      81        81\n",
       "청구관리               56     56      56        56\n",
       "통계                218    218     218       218"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "lesser-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 모델은 sentence와 label만 써보자\n",
    "df_zip = df[ ['sentence', 'label'] ]\n",
    "\n",
    "y = df_zip.pop('label')\n",
    "X = df_zip.pop('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "composed-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열로 되어 있는 label을 categorical value로 변환\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "mediterranean-crystal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소속영업장 변경 요청(SKB사내유치본점 -＞ 엘에스통신-채널고객팀) . SR-1705-0886;- 해당 서비스번호로 개통이 됐다고 하는데 스윙에서 서비스 번호로 검색이 되지 않아서 ;변경이 어렵습니다.;- 유통지원센터에서 청약 등록 시, 창리정보통신으로 등록했으나 스윙 이관후 확인 시 소속영업장이 금란텔레콤으로 되어 있어 변경 요청 함 . 요청유형:자료수정,요청유형상세:PRM,검토/승인자성명:홍도희,검토/승인자사번:1700,검토승인자기간:2017-05-31,요청내용:1. 변경 전 유통망 : SKB사내유치본점2. 변경 후 유통망 : 엘에스통신(E00901)3. 서비스번호 : 7276564018 또는 1670-84914. 고객 : 세종화재해상자동차손해사정(주)5. 담당 AM : 박성M6. 적용 시점 : 6월 지급 분부터 적용(5월 영업에 대한)\n",
      "\n",
      "소속영업장 변경 요청 SKB사내유치본점 엘에스통신 채널고객팀 . SR 해당 서비스번호로 개통이 됐다고 하는데 스윙에서 서비스 번호로 검색이 되지 않아서 변경이 어렵습니다. 유통지원센터에서 청약 등록 시 창리정보통신으로 등록했으나 스윙 이관후 확인 시 소속영업장이 금란텔레콤으로 되어 있어 변경 요청 함 . 요청유형 자료수정 요청유형상세 PRM 검토/승인자성명 홍도희 검토/승인자사번 검토승인자기간 요청내용 . 변경 전 유통망 SKB사내유치본점 . 변경 후 유통망 엘에스통신 E . 서비스번호 또는 . 고객 세종화재해상자동차손해사정 주 . 담당 AM 박성M . 적용 시점 월 지급 분부터 적용 월 영업에 대한 \n",
      "\n",
      "['소속영업장 변경 요청 SKB사내유치본점 엘에스통신 채널고객팀', 'SR 해당 서비스번호로 개통이 됐다고 하는데 스윙에서 서비스 번호로 검색이 되지 않아서 변경이 어렵습니다', '유통지원센터에서 청약 등록 시 창리정보통신으로 등록했으나 스윙 이관후 확인 시 소속영업장이 금란텔레콤으로 되어 있어 변경 요청 함', '요청유형 자료수정 요청유형상세 PRM 검토/승인자성명 홍도희 검토/승인자사번 검토승인자기간 요청내용', '변경 전 유통망 SKB사내유치본점', '변경 후 유통망 엘에스통신 E', '서비스번호 또는', '고객 세종화재해상자동차손해사정 주', '담당 AM 박성M', '적용 시점 월 지급 분부터 적용 월 영업에 대한']\n",
      "\n",
      "소속 영업장 변경 요청 SKB 사내 유치 본점 에스 통신 채널 고객 팀 SR 해당 서비스 번호 개통 스윙 서비스 번호 검색 변경 어렵 유통 지원 센터 청약 등록 시 창리 정보 통신 등록 스윙 소속 영업장 금란 텔레콤 변경 요청 요청 유형 자료 수정 요청 유형 상세 PRM 검토 승인 성명 홍도 희 검토 승인 사 번 검토 승인 자기 요청 내용 변경 전 유통망 SKB 사내 유치 본점 변경 후 유통망 에스 통신 E 서비스 번호 고객 세종 해상 자동차 손해 사정 주 담당 AM 박성 M 적용 시점 월 지급 적용 월 영업\n",
      "[1484, 1872, 10198, 1815, 1124, 587, 10198, 3997, 2756, 10198, 10198, 745, 10198, 147, 731, 102, 10198, 7, 107, 10198, 1026, 10198, 10198, 8074, 298, 10198, 10198, 10198, 10198, 2480, 1375, 10198, 3249, 4622, 10198, 8975, 1159, 10198, 71, 10198, 147, 418, 10198, 899, 2265, 10198, 3715, 1769, 10198, 10198, 10, 362, 10198, 137, 2480, 10198, 10198, 6601, 10198, 3715, 1769, 10198, 10198, 10, 362, 10198, 5760, 3432, 10198, 3997, 2756, 10198, 10198, 10198, 10198, 1026, 2480, 10198, 1597, 92, 10198, 9750, 1271, 10198, 745, 1174, 10198, 79, 2598, 10198, 25, 10198, 943, 1175, 10198, 1015, 2068, 10198, 2480, 1375, 10198, 79, 2598, 10198, 10198, 6601, 10198, 1484, 1872, 10198, 1815, 1124, 587, 10198, 610, 3298, 10198, 780, 10198, 1640, 10198, 3997, 2756, 10198, 10198, 745, 10198, 10198, 745, 10198, 1026, 3042, 10198, 1228, 551, 10198, 91, 1015, 10198, 10198, 745, 10198, 1026, 3042, 10198, 2325, 1000, 10198, 277, 418, 86, 10198, 5760, 4302, 10198, 1910, 710, 10198, 2119, 47, 10198, 2563, 4404, 10198, 1247, 10198, 5760, 4302, 10198, 1910, 710, 10198, 7, 10198, 10, 10198, 5760, 4302, 10198, 1910, 710, 10198, 1228, 402, 10198, 10198, 745, 10198, 107, 7017, 10198, 3997, 2756, 10198, 30, 10198, 1026, 2480, 513, 10198, 147, 731, 102, 10198, 7, 107, 10198, 1026, 10198, 10198, 8074, 298, 10198, 3997, 2756, 10198, 19, 10198, 1026, 2480, 513, 10198, 10198, 10198, 10198, 2480, 1375, 10198, 274, 10198, 3715, 1769, 10198, 10198, 10, 362, 10198, 8975, 1159, 10198, 1000, 755, 10198, 899, 2325, 10198, 1228, 802, 189, 10198, 894, 899, 10198, 7, 1015, 10198, 291, 10198, 1637, 2265, 10198, 129, 86, 10198, 1202, 2119, 10198, 86, 10198, 2412, 7017, 10198, 25, 298, 10198, 15, 10198, 1597, 1646, 10198, 2412, 7017, 10198, 15, 10198, 1815, 1124]\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "print( X[0] )\n",
    "print()\n",
    "cleansed_text = tl.clean_text( X[0] )\n",
    "print( cleansed_text )\n",
    "print()\n",
    "tokenized_sentence = tl.segment_sentences(cleansed_text)\n",
    "print( tokenized_sentence )\n",
    "print()\n",
    "corpora = ' '.join(tl.get_corpora(tokenized_sentence))\n",
    "print(corpora)\n",
    "sequence = [w2v_output['word2index'][t] if t in w2v_output['word2index'] \n",
    "                                        else w2v_output['word2index'][ UNK_TOKEN ]\n",
    "                                        for t in corpora]\n",
    "\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "future-button",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92197 개의 데이터 존재 확인!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-0e54f3bb8afb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegment_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleansed_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# 문장 배열을 입릭으로 받아 다시 하나의 문자열로 변환하여 X에 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtokenized_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_corpora\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     sequence = [w2v_output['word2index'][t] if t in w2v_output['word2index'] \n\u001b[0;32m     17\u001b[0m                                             \u001b[1;32melse\u001b[0m \u001b[0mw2v_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word2index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mUNK_TOKEN\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 문장 전처리\n",
    "preprocessed_X = []\n",
    "print(f'{len(X)} 개의 데이터 존재 확인!')\n",
    "for i, text in enumerate(X):\n",
    "    try:\n",
    "        # 클렌징\n",
    "        cleansed_text = tl.clean_text(text)\n",
    "    except TypeError:\n",
    "        print(f'      {i+1} 번째 데이터에 문제가 있어 skip!')\n",
    "        continue\n",
    "\n",
    "    # 문장으로 분리하여 배열로 리턴\n",
    "    sentences = tl.segment_sentences(cleansed_text)\n",
    "    # 문장 배열을 입릭으로 받아 다시 하나의 문자열로 변환하여 X에 저장\n",
    "    tokenized_sentence = tl.get_corpora(sentences)\n",
    "    sequence = [w2v_output['word2index'][t] if t in w2v_output['word2index'] \n",
    "                                            else w2v_output['word2index'][ UNK_TOKEN ]\n",
    "                                            for t in tokenized_sentence]\n",
    "    preprocessed_X.append(sequence)\n",
    "\n",
    "    if i % 5000 == 0 and i > 0:\n",
    "        print(f'      {i} 번째 데이터 처리 완료!')\n",
    "\n",
    "preprocessed_X = pad_sequences( preprocessed_X, maxlen=50, padding='post', value=w2v_output['word2index'][PAD_TOKEN] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "previous-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(preprocessed_X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "solar-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64537\n",
      "27660\n",
      "64537\n",
      "27660\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test  = np.array(X_test)\n",
    "y_test  = np.array(y_test)\n",
    "\n",
    "print( len(X_train) )\n",
    "print( len(X_test) )\n",
    "print( len(y_train) )\n",
    "print( len(y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "rational-february",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "broad-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embedding_dim = w2v_output['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "hybrid-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        # (batch, )\n",
    "        self.w1 = Dense(units, activation='tanh')\n",
    "        self.w2 = Dense(1)    \n",
    "        self.softmax_ = Softmax(axis=1)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # (batch, seq, embedding_dim*2) -> # (batch, seq, dec_units)\n",
    "        x = self.w1(x)\n",
    "        # (batch, seq, dec_units) -> # (batch, seq, 1)\n",
    "        score = self.softmax_( self.w2(x) )\n",
    "\n",
    "        return tf.squeeze( tf.matmul(tf.transpose(x, perm=[0, 2, 1]), score ), axis=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "sunset-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoviePosNegClassifier(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, \n",
    "                 batch_size, embedding_weights, apply_attention, train_embedding_layer, dropout, classes):\n",
    "        super(MoviePosNegClassifier, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            weights=[embedding_weights])\n",
    "        self.lstm = Bidirectional( LSTM(self.dec_units, return_sequences=True) )\n",
    "        self.fc1 = Dense(128, activation='relu')\n",
    "        self.fc2 = Dense(classes, activation='softmax')\n",
    "        self.do1 = Dropout(dropout)\n",
    "        self.do2 = Dropout(dropout)\n",
    "        \n",
    "        self.attention = Attention(self.dec_units)\n",
    "        self.embedding.trainable = train_embedding_layer\n",
    "        self.apply_attention = apply_attention\n",
    "        \n",
    "    def call(self, x):\n",
    "        # (batch, seq) -> (batch, seq, embedding_dim)        \n",
    "        x = self.embedding(x)\n",
    "        x = self.do1(x)\n",
    "        # (batch, seq, embedding_dim) -> (batch, seq, embedding_dim*2)        \n",
    "        x = self.lstm(x)\n",
    "        \n",
    "        # (batch, seq, embedding_dim*2) -> (batch, embedding_dim)        \n",
    "        if self.apply_attention:\n",
    "            x = self.attention(x)\n",
    "        \n",
    "        # (batch, embedding_dim) -> (batch, 128)    \n",
    "        x = self.fc1(x)\n",
    "        x = self.do2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "geological-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(vocab_size, embedding_dim, dec_units, epochs, batch_size, \n",
    "       weights, apply_attention, train_embedding_layer, dropout, classes):\n",
    "    model = MoviePosNegClassifier(\n",
    "        vocab_size, \n",
    "        embedding_dim, \n",
    "        dec_units, \n",
    "        batch_size,\n",
    "        weights,\n",
    "        apply_attention,\n",
    "        train_embedding_layer,\n",
    "        dropout,\n",
    "        classes\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "    \n",
    "    test_score = model.evaluate(X_test, y_test, verbose=2)\n",
    "    \n",
    "    plot_hist(history)\n",
    "    \n",
    "    return f'{apply_attention}-{train_embedding_layer}-{dropout}', history, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "offshore-scholarship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": " Cast string to int32 is not supported\n\t [[node movie_pos_neg_classifier_2/embedding_2/Cast (defined at <ipython-input-68-36efa8b1f22a>:23) ]] [Op:__inference_train_function_10352]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node movie_pos_neg_classifier_2/embedding_2/Cast:\n ExpandDims (defined at <ipython-input-80-a1029fdb7fb5>:20)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-4b7744f6bf25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_score\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{model_name}-{i}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mhistories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-a1029fdb7fb5>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(vocab_size, embedding_dim, dec_units, epochs, batch_size, weights, apply_attention, train_embedding_layer, dropout, classes)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     history = model.fit(\n\u001b[1;32m---> 20\u001b[1;33m         X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m:  Cast string to int32 is not supported\n\t [[node movie_pos_neg_classifier_2/embedding_2/Cast (defined at <ipython-input-68-36efa8b1f22a>:23) ]] [Op:__inference_train_function_10352]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node movie_pos_neg_classifier_2/embedding_2/Cast:\n ExpandDims (defined at <ipython-input-80-a1029fdb7fb5>:20)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    'apply_attention': [True, False],\n",
    "    'train_embedding_layer': [True, False],\n",
    "    'dropout': [0.3],\n",
    "    'weights': [w2v_output['weight'], w2v_output['norm_weight']]\n",
    "}\n",
    "\n",
    "histories = dict()\n",
    "test_scores = dict()\n",
    "\n",
    "for a in hyper_params['apply_attention']:\n",
    "    for ef in hyper_params['train_embedding_layer']:\n",
    "        for do in hyper_params['dropout']:\n",
    "            for i, w in enumerate(hyper_params['weights']):\n",
    "                model_name, history, test_score = \\\n",
    "                    train_and_evaluate(vocab_size, embedding_dim, 128, 10, 128, w, a, ef, do, len(label_encoder.classes_))\n",
    "                model_name = f'{model_name}-{i}'\n",
    "                histories[model_name] = history\n",
    "                test_scores[model_name] = test_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
